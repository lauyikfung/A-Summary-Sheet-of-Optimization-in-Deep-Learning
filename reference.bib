@INPROCEEDINGS{prop,
  author={Riedmiller, M. and Braun, H.},
  booktitle={IEEE International Conference on Neural Networks}, 
  title={A direct adaptive method for faster backpropagation learning: the RPROP algorithm}, 
  year={1993},
  volume={},
  number={},
  pages={586-591 vol.1},
  keywords={Backpropagation algorithms;Neurons;Acceleration;Supervised learning;Feedforward systems;Computer networks;Convergence;Writing},
  doi={10.1109/ICNN.1993.298623}}

@inproceedings{bernstein2018signsgd,
  title={signSGD: Compressed optimisation for non-convex problems},
  author={Bernstein, Jeremy and Wang, Yu-Xiang and Azizzadenesheli, Kamyar and Anandkumar, Animashree},
  booktitle={International Conference on Machine Learning},
  pages={560--569},
  year={2018},
  organization={PMLR}
}
@inproceedings{chen2024symbolic,
  author       = {Xiangning Chen and
                  Chen Liang and
                  Da Huang and
                  Esteban Real and
                  Kaiyuan Wang and
                  Hieu Pham and
                  Xuanyi Dong and
                  Thang Luong and
                  Cho{-}Jui Hsieh and
                  Yifeng Lu and
                  Quoc V. Le},
  editor       = {Alice Oh and
                  Tristan Naumann and
                  Amir Globerson and
                  Kate Saenko and
                  Moritz Hardt and
                  Sergey Levine},
  title        = {Symbolic Discovery of Optimization Algorithms},
  booktitle    = {Advances in Neural Information Processing Systems 36: Annual Conference
                  on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans,
                  LA, USA, December 10 - 16, 2023},
  year         = {2023},
}
@inproceedings{rumelhart1986learning,
  author       = {Marcus Grum},
  editor       = {Leszek Rutkowski and
                  Rafal Scherer and
                  Marcin Korytkowski and
                  Witold Pedrycz and
                  Ryszard Tadeusiewicz and
                  Jacek M. Zurada},
  title        = {Learning Representations by Crystallized Back-Propagating Errors},
  booktitle    = {Artificial Intelligence and Soft Computing - 22nd International Conference,
                  {ICAISC} 2023, Zakopane, Poland, June 18-22, 2023, Proceedings, Part
                  {I}},
  series       = {Lecture Notes in Computer Science},
  volume       = {14125},
  pages        = {78--100},
  publisher    = {Springer},
  year         = {2023},
  url          = {https://doi.org/10.1007/978-3-031-42505-9\_8},
  doi          = {10.1007/978-3-031-42505-9\_8},
}
@inproceedings{nesterov1983method,
  title={A method for solving the convex programming problem with convergence rate O (1/k2)},
  author={Nesterov, Yurii},
  booktitle={Dokl akad nauk Sssr},
  volume={269},
  pages={543},
  year={1983}
}
@article{duchi2011adaptive,
  title={Adaptive subgradient methods for online learning and stochastic optimization.},
  author={Duchi, John and Hazan, Elad and Singer, Yoram},
  journal={Journal of machine learning research},
  volume={12},
  number={7},
  year={2011}
}
@article{zeiler2012adadelta,
  title={ADADELTA: an adaptive learning rate method},
  author={Zeiler, Matthew D},
  journal={arXiv preprint arXiv:1212.5701},
  year={2012}
}
@article{RMSProp,
	title={Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude},
	author={Tieleman, Tijmen and Hinton, Geoffrey},
	journal={COURSERA: Neural networks for machine learning},
	volume={4},
	number={2},
	pages={26--31},
	year={2012}
}
@inproceedings{kingma2014adam,
  author       = {Diederik P. Kingma and
                  Jimmy Ba},
  editor       = {Yoshua Bengio and
                  Yann LeCun},
  title        = {Adam: {A} Method for Stochastic Optimization},
  booktitle    = {3rd International Conference on Learning Representations, {ICLR} 2015,
                  San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  year         = {2015},
}
@inproceedings{loshchilov2017decoupled,
  author       = {Ilya Loshchilov and
                  Frank Hutter},
  title        = {Decoupled Weight Decay Regularization},
  booktitle    = {7th International Conference on Learning Representations, {ICLR} 2019,
                  New Orleans, LA, USA, May 6-9, 2019},
  year         = {2019},
}
@inproceedings{reddi2019convergence,
  author       = {Sashank J. Reddi and
                  Satyen Kale and
                  Sanjiv Kumar},
  title        = {On the Convergence of Adam and Beyond},
  booktitle    = {6th International Conference on Learning Representations, {ICLR} 2018,
                  Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings},
  year         = {2018},
}
@article{dozat2016incorporating,
  title={Incorporating nesterov momentum into adam},
  author={Dozat, Timothy},
  year={2016}
}
@article{chen2018padam,
  title={Padam: Closing the generalization gap of adaptive gradient methods in training deep neural networks},
  author={Chen, Jinghui and Gu, Quanquan},
  year={2018}
}
@article{liu2020adam,
  title={Adam $^+$: A Stochastic Method with Adaptive Variance Reduction},
  author={Liu, Mingrui and Zhang, Wei and Orabona, Francesco and Yang, Tianbao},
  journal={arXiv preprint arXiv:2011.11985},
  year={2020}
}
@article{you2017large,
  title={Large batch training of convolutional networks},
  author={You, Yang and Gitman, Igor and Ginsburg, Boris},
  journal={arXiv preprint arXiv:1708.03888},
  year={2017}
}
@inproceedings{you2019large2,
  author       = {Yang You and
                  Jing Li and
                  Sashank J. Reddi and
                  Jonathan Hseu and
                  Sanjiv Kumar and
                  Srinadh Bhojanapalli and
                  Xiaodan Song and
                  James Demmel and
                  Kurt Keutzer and
                  Cho{-}Jui Hsieh},
  title        = {Large Batch Optimization for Deep Learning: Training {BERT} in 76
                  minutes},
  booktitle    = {8th International Conference on Learning Representations, {ICLR} 2020,
                  Addis Ababa, Ethiopia, April 26-30, 2020},
  year         = {2020},
}
@inproceedings{yuan2024mars,
  title={MARS: Unleashing the Power of Variance Reduction for Training Large Models},
  author={Yuan, Huizhuo and Liu, Yifeng and Wu, Shuang and Gu, Quanquan and others},
  booktitle={Forty-second International Conference on Machine Learning},
  year={2025}
}
@article{li2020adax,
  title={Adax: Adaptive gradient descent with exponential long term memory},
  author={Li, Wenjie and Zhang, Zhaoyang and Wang, Xinjiang and Luo, Ping},
  journal={arXiv preprint arXiv:2004.09740},
  year={2020}
}
@article{xie2024adan,
  title={Adan: Adaptive nesterov momentum algorithm for faster optimizing deep models},
  author={Xie, Xingyu and Zhou, Pan and Li, Huan and Lin, Zhouchen and Yan, Shuicheng},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2024},
  publisher={IEEE}
}
@article{defazio2024road,
  title={The road less scheduled},
  author={Defazio, Aaron and Yang, Xingyu Alice and Mehta, Harsh and Mishchenko, Konstantin and Khaled, Ahmed and Cutkosky, Ashok},
  journal={arXiv preprint arXiv:2405.15682},
  year={2024}
}
@article{loshchilov2023weight,
  title={Weight Norm Control},
  author={Loshchilov, Ilya},
  journal={arXiv preprint arXiv:2311.11446},
  year={2023}
}
@article{zhuang2020adabelief,
  title={Adabelief optimizer: Adapting stepsizes by the belief in observed gradients},
  author={Zhuang, Juntang and Tang, Tommy and Ding, Yifan and Tatikonda, Sekhar C and Dvornek, Nicha and Papademetris, Xenophon and Duncan, James},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={18795--18806},
  year={2020}
}
@inproceedings{li2022root,
  title={Root-sgd: Sharp nonasymptotics and asymptotic efficiency in a single algorithm},
  author={Li, Chris Junchi and Mou, Wenlong and Wainwright, Martin and Jordan, Michael},
  booktitle={Conference on Learning Theory},
  pages={909--981},
  year={2022},
  organization={PMLR}
}
@article{huang2021super,
  title={Super-adam: faster and universal framework of adaptive gradients},
  author={Huang, Feihu and Li, Junyi and Huang, Heng},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={9074--9085},
  year={2021}
}
@article{hutchinson1989stochastic,
  title={A stochastic estimator of the trace of the influence matrix for Laplacian smoothing splines},
  author={Hutchinson, Michael F},
  journal={Communications in Statistics-Simulation and Computation},
  volume={18},
  number={3},
  pages={1059--1076},
  year={1989},
  publisher={Taylor \& Francis}
}
@inproceedings{yao2021adahessian,
  title={Adahessian: An adaptive second order optimizer for machine learning},
  author={Yao, Zhewei and Gholami, Amir and Shen, Sheng and Mustafa, Mustafa and Keutzer, Kurt and Mahoney, Michael},
  booktitle={proceedings of the AAAI conference on artificial intelligence},
  volume={35},
  number={12},
  pages={10665--10673},
  year={2021}
}
@article{cutkosky2019momentum,
  title={Momentum-based variance reduction in non-convex sgd},
  author={Cutkosky, Ashok and Orabona, Francesco},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}
@article{levy2021storm+,
  title={Storm+: Fully adaptive sgd with recursive momentum for nonconvex optimization},
  author={Levy, Kfir and Kavis, Ali and Cevher, Volkan},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={20571--20582},
  year={2021}
}
@inproceedings{liu2023sophia,
  author       = {Hong Liu and
                  Zhiyuan Li and
                  David Leo Wright Hall and
                  Percy Liang and
                  Tengyu Ma},
  title        = {Sophia: {A} Scalable Stochastic Second-order Optimizer for Language
                  Model Pre-training},
  booktitle    = {The Twelfth International Conference on Learning Representations,
                  {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  year         = {2024},
}
@article{tian2022amos,
  title={Amos: An adam-style optimizer with adaptive weight decay towards model-oriented scale},
  author={Tian, Ran and Parikh, Ankur P},
  journal={arXiv preprint arXiv:2210.11693},
  year={2022}
}
@inproceedings{martens2010deep,
  title={Deep learning via hessian-free optimization.},
  author={Martens, James and others},
  booktitle={Icml},
  volume={27},
  pages={735--742},
  year={2010}
}
@inproceedings{martens2015optimizing,
  title={Optimizing neural networks with kronecker-factored approximate curvature},
  author={Martens, James and Grosse, Roger},
  booktitle={International conference on machine learning},
  pages={2408--2417},
  year={2015},
  organization={PMLR}
}
@inproceedings{gupta2018shampoo,
  title={Shampoo: Preconditioned stochastic tensor optimization},
  author={Gupta, Vineet and Koren, Tomer and Singer, Yoram},
  booktitle={International Conference on Machine Learning},
  pages={1842--1850},
  year={2018},
  organization={PMLR}
}
@article{agarwal2020disentangling,
  title={Disentangling adaptive gradient methods from learning rates},
  author={Agarwal, Naman and Anil, Rohan and Hazan, Elad and Koren, Tomer and Zhang, Cyril},
  journal={arXiv preprint arXiv:2002.11803},
  year={2020}
}
@article{zhang2024adam,
  title={Adam-mini: Use fewer learning rates to gain more},
  author={Zhang, Yushun and Chen, Congliang and Li, Ziniu and Ding, Tian and Wu, Chenwei and Ye, Yinyu and Luo, Zhi-Quan and Sun, Ruoyu},
  journal={arXiv preprint arXiv:2406.16793},
  year={2024}
}
@inproceedings{zhao2024galore,
  author       = {Jiawei Zhao and
                  Zhenyu Zhang and
                  Beidi Chen and
                  Zhangyang Wang and
                  Anima Anandkumar and
                  Yuandong Tian},
  title        = {GaLore: Memory-Efficient {LLM} Training by Gradient Low-Rank Projection},
  booktitle    = {Forty-first International Conference on Machine Learning, {ICML} 2024,
                  Vienna, Austria, July 21-27, 2024},
  year         = {2024},
}
@article{yang2022tensor,
  title={Tensor programs v: Tuning large neural networks via zero-shot hyperparameter transfer},
  author={Yang, Greg and Hu, Edward J and Babuschkin, Igor and Sidor, Szymon and Liu, Xiaodong and Farhi, David and Ryder, Nick and Pachocki, Jakub and Chen, Weizhu and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2203.03466},
  year={2022}
}
@misc{tigeropt,
  title={Tiger: A Tight-fisted Optimizer},
  author={Jianlin Su},
  year={2023},
  howpublished={\url{https://github.com/bojone/tiger}},
}
@inproceedings{defazio2023learning,
  title={Learning-rate-free learning by d-adaptation},
  author={Defazio, Aaron and Mishchenko, Konstantin},
  booktitle={International Conference on Machine Learning},
  pages={7449--7479},
  year={2023},
  organization={PMLR}
}
@article{bernstein2024old,
  title={Old optimizer, new norm: An anthology},
  author={Bernstein, Jeremy and Newhouse, Laker},
  journal={arXiv preprint arXiv:2409.20325},
  year={2024}
}
@misc{higham2008functions,
  title={Functions of Matrices. Society for Industrial and Applied Mathematics},
  author={Higham, Nicholas J},
  year={2008},
  publisher={SIAM Philadelphia},
  page={15}
}
@article{tran2019convergence,
  title={On the convergence proof of amsgrad and a new version},
  author={Tran, Phuong Thi and others},
  journal={IEEE Access},
  volume={7},
  pages={61706--61716},
  year={2019},
  publisher={IEEE}
}
@article{ward2020adagrad,
  title={Adagrad stepsizes: Sharp convergence over nonconvex landscapes},
  author={Ward, Rachel and Wu, Xiaoxia and Bottou, Leon},
  journal={Journal of Machine Learning Research},
  volume={21},
  number={219},
  pages={1--30},
  year={2020}
}
@article{jiang2024adaptive,
  title={Adaptive SGD with Polyak stepsize and line-search: Robust convergence and variance reduction},
  author={Jiang, Xiaowen and Stich, Sebastian U},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@misc{jordan2024muon,
  author       = {Keller Jordan and Yuchen Jin and Vlado Boza and Jiacheng You and
                  Franz Cecista and Laker Newhouse and Jeremy Bernstein},
  title        = {Muon: An optimizer for hidden layers in neural networks},
  year         = {2024},
  url          = {https://kellerjordan.github.io/posts/muon/}
}
@article{tran2019hybrid,
  title={Hybrid stochastic gradient descent algorithms for stochastic nonconvex optimization},
  author={Tran-Dinh, Quoc and Pham, Nhan H and Phan, Dzung T and Nguyen, Lam M},
  journal={arXiv preprint arXiv:1905.05920},
  year={2019}
}
@article{pagliardini2024ademamix,
  title={The ademamix optimizer: Better, faster, older},
  author={Pagliardini, Matteo and Ablin, Pierre and Grangier, David},
  journal={arXiv preprint arXiv:2409.03137},
  year={2024}
}
@article{armijo1966minimization,
  title={Minimization of functions having Lipschitz continuous first partial derivatives},
  author={Armijo, Larry},
  journal={Pacific Journal of mathematics},
  volume={16},
  number={1},
  pages={1--3},
  year={1966},
  publisher={Mathematical Sciences Publishers}
}
@article{scheithauerjorge,
  title={Numerical Optimization},
  author={Jorge Nocedal and Stephen J. Wright},
  journal={},
  volume ={Springer, New York, NY, USA, 2e edition},
  year={2006}
}
@article{vyas2024soap,
  title={Soap: Improving and stabilizing shampoo using adam},
  author={Vyas, Nikhil and Morwani, Depen and Zhao, Rosie and Shapira, Itai and Brandfonbrener, David and Janson, Lucas and Kakade, Sham},
  journal={arXiv preprint arXiv:2409.11321},
  year={2024}
}
@article{roux2012stochastic,
  title={A stochastic gradient method with an exponential convergence \_rate for finite training sets},
  author={Roux, Nicolas and Schmidt, Mark and Bach, Francis},
  journal={Advances in neural information processing systems},
  volume={25},
  year={2012}
}
@article{shalev2013stochastic,
  title={Stochastic dual coordinate ascent methods for regularized loss minimization.},
  author={Shalev-Shwartz, Shai and Zhang, Tong},
  journal={Journal of Machine Learning Research},
  volume={14},
  number={1},
  year={2013}
}
@article{johnson2013accelerating,
  title={Accelerating stochastic gradient descent using predictive variance reduction},
  author={Johnson, Rie and Zhang, Tong},
  journal={Advances in neural information processing systems},
  volume={26},
  year={2013}
}
@article{defazio2014saga,
  title={SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives},
  author={Defazio, Aaron and Bach, Francis and Lacoste-Julien, Simon},
  journal={Advances in neural information processing systems},
  volume={27},
  year={2014}
}
@inproceedings{nguyen2017sarah,
  title={SARAH: A novel method for machine learning problems using stochastic recursive gradient},
  author={Nguyen, Lam M and Liu, Jie and Scheinberg, Katya and Tak{\'a}{\v{c}}, Martin},
  booktitle={International conference on machine learning},
  pages={2613--2621},
  year={2017},
  organization={PMLR}
}
@article{fang2018spider,
  title={Spider: Near-optimal non-convex optimization via stochastic path-integrated differential estimator},
  author={Fang, Cong and Li, Chris Junchi and Lin, Zhouchen and Zhang, Tong},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}
@article{zhou2020stochastic,
  title={Stochastic nested variance reduction for nonconvex optimization},
  author={Zhou, Dongruo and Xu, Pan and Gu, Quanquan},
  journal={Journal of machine learning research},
  volume={21},
  number={103},
  pages={1--63},
  year={2020}
}
@article{wang2019spiderboost,
  title={Spiderboost and momentum: Faster variance reduction algorithms},
  author={Wang, Zhe and Ji, Kaiyi and Zhou, Yi and Liang, Yingbin and Tarokh, Vahid},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}
@inproceedings{shazeer2018adafactor,
  title={Adafactor: Adaptive learning rates with sublinear memory cost},
  author={Shazeer, Noam and Stern, Mitchell},
  booktitle={International Conference on Machine Learning},
  pages={4596--4604},
  year={2018},
  organization={PMLR}
}
@article{polyak1992acceleration,
  title={Acceleration of stochastic approximation by averaging},
  author={Polyak, Boris T and Juditsky, Anatoli B},
  journal={SIAM journal on control and optimization},
  volume={30},
  number={4},
  pages={838--855},
  year={1992},
  publisher={SIAM}
}
@article{liu1989limited,
  title={On the limited memory BFGS method for large scale optimization},
  author={Liu, Dong C and Nocedal, Jorge},
  journal={Mathematical programming},
  volume={45},
  number={1},
  pages={503--528},
  year={1989},
  publisher={Springer}
}
@article{zaheer2018adaptive,
  title={Adaptive methods for nonconvex optimization},
  author={Zaheer, Manzil and Reddi, Sashank and Sachan, Devendra and Kale, Satyen and Kumar, Sanjiv},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}
@article{zhang2021dive,
  title={Dive into deep learning},
  author={Zhang, Aston and Lipton, Zachary C and Li, Mu and Smola, Alexander J},
  journal={arXiv preprint arXiv:2106.11342},
  year={2021},
  pages={505}
}
@inproceedings{liu2019variance,
  author       = {Liyuan Liu and
                  Haoming Jiang and
                  Pengcheng He and
                  Weizhu Chen and
                  Xiaodong Liu and
                  Jianfeng Gao and
                  Jiawei Han},
  title        = {On the Variance of the Adaptive Learning Rate and Beyond},
  booktitle    = {8th International Conference on Learning Representations, {ICLR} 2020,
                  Addis Ababa, Ethiopia, April 26-30, 2020},
  year         = {2020},
}
@article{zhao2024deconstructing,
  title={Deconstructing what makes a good optimizer for language models},
  author={Zhao, Rosie and Morwani, Depen and Brandfonbrener, David and Vyas, Nikhil and Kakade, Sham},
  journal={arXiv preprint arXiv:2407.07972},
  year={2024}
}
@article{yuan2020stochastic,
  title={Stochastic recursive momentum for policy gradient methods},
  author={Yuan, Huizhuo and Lian, Xiangru and Liu, Ji and Zhou, Yuren},
  journal={arXiv preprint arXiv:2003.04302},
  year={2020}
}
@book{fletcher1987practical,
  title={Practical Methods of Optimization},
  author={Fletcher, Roger},
  year={1987},
  edition={2},
  publisher={John Wiley \& Sons},
  address={New York},
  isbn={978-0-471-91547-8}
}
@inproceedings{dalalyan2017further,
  title={Further and stronger analogy between sampling and optimization: Langevin Monte Carlo and gradient descent},
  author={Dalalyan, Arnak},
  booktitle={Conference on Learning Theory},
  pages={678--689},
  year={2017},
  organization={PMLR}
}
@article{dalalyan2017theoretical,
  title={Theoretical guarantees for approximate sampling from smooth and log-concave densities},
  author={Dalalyan, Arnak S},
  journal={Journal of the Royal Statistical Society Series B: Statistical Methodology},
  volume={79},
  number={3},
  pages={651--676},
  year={2017},
  publisher={Oxford University Press}
}
@article{durmus2017nonasymptotic,
  title={Nonasymptotic convergence analysis for the unadjusted Langevin algorithm},
  author={Durmus, Alain and Moulines, {\'E}ric},
  journal={The Annals of Applied Probability},
  pages={1551--1587},
  year={2017},
  publisher={JSTOR}
}
@inproceedings{welling2011bayesian,
  title={Bayesian learning via stochastic gradient Langevin dynamics},
  author={Welling, Max and Teh, Yee W},
  booktitle={Proceedings of the 28th international conference on machine learning (ICML-11)},
  pages={681--688},
  year={2011},
  organization={Citeseer}
}
@article{xu2018global,
  title={Global convergence of Langevin dynamics based algorithms for nonconvex optimization},
  author={Xu, Pan and Chen, Jinghui and Zou, Difan and Gu, Quanquan},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}
@inproceedings{ahn2012bayesian,
  author       = {Sungjin Ahn and
                  Anoop Korattikara Balan and
                  Max Welling},
  title        = {Bayesian Posterior Sampling via Stochastic Gradient Fisher Scoring},
  booktitle    = {Proceedings of the 29th International Conference on Machine Learning,
                  {ICML} 2012, Edinburgh, Scotland, UK, June 26 - July 1, 2012},
  publisher    = {icml.cc / Omnipress},
  year         = {2012},
  url          = {http://icml.cc/2012/papers/782.pdf},
  timestamp    = {Wed, 03 Apr 2019 17:43:35 +0200},
  biburl       = {https://dblp.org/rec/conf/icml/AhnBW12.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{wu2024self,
  title={Self-play preference optimization for language model alignment},
  author={Wu, Yue and Sun, Zhiqing and Yuan, Huizhuo and Ji, Kaixuan and Yang, Yiming and Gu, Quanquan},
  journal={arXiv preprint arXiv:2405.00675},
  year={2024}
}
@misc{tao2024simpleprovableparameterfreeadaptive,
      title={Towards Simple and Provable Parameter-Free Adaptive Gradient Methods}, 
      author={Yuanzhe Tao and Huizhuo Yuan and Xun Zhou and Yuan Cao and Quanquan Gu},
      year={2024},
      eprint={2412.19444},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2412.19444}, 
}
@article{liang2024cautious,
  title={Cautious Optimizers: Improving Training with One Line of Code},
  author={Liang, Kaizhao and Chen, Lizhang and Liu, Bo and Liu, Qiang},
  journal={arXiv preprint arXiv:2411.16085},
  year={2024}
}
@article{cao2024grams,
  title={Grams: Gradient Descent with Adaptive Momentum Scaling},
  author={Cao, Yang and Li, Xiaoyu and Song, Zhao},
  journal={arXiv preprint arXiv:2412.17107},
  year={2024}
}
@article{defazio2023and,
  title={When, why and how much? adaptive learning rate scheduling by refinement},
  author={Defazio, Aaron and Cutkosky, Ashok and Mehta, Harsh and Mishchenko, Konstantin},
  journal={arXiv preprint arXiv:2310.07831},
  year={2023}
}
@article{hu2024minicpm,
  title={Minicpm: Unveiling the potential of small language models with scalable training strategies},
  author={Hu, Shengding and Tu, Yuge and Han, Xu and He, Chaoqun and Cui, Ganqu and Long, Xiang and Zheng, Zhi and Fang, Yewei and Huang, Yuxiang and Zhao, Weilin and others},
  journal={arXiv preprint arXiv:2404.06395},
  year={2024}
}
@article{wen2024understanding,
  title={Understanding Warmup-Stable-Decay Learning Rates: A River Valley Loss Landscape Perspective},
  author={Wen, Kaiyue and Li, Zhiyuan and Wang, Jason and Hall, David and Liang, Percy and Ma, Tengyu},
  journal={arXiv preprint arXiv:2410.05192},
  year={2024}
}
@article{li2023convergence,
  title={Convergence of adam under relaxed assumptions},
  author={Li, Haochuan and Rakhlin, Alexander and Jadbabaie, Ali},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={52166--52196},
  year={2023}
}
@article{an2025asgo,
  title={Asgo: Adaptive structured gradient optimization},
  author={An, Kang and Liu, Yuxing and Pan, Rui and Ma, Shiqian and Goldfarb, Donald and Zhang, Tong},
  journal={arXiv preprint arXiv:2503.20762},
  year={2025}
}
@article{liu2025cosmos,
  title={Cosmos: A hybrid adaptive optimizer for memory-efficient training of llms},
  author={Liu, Liming and Xu, Zhenghao and Zhang, Zixuan and Kang, Hao and Li, Zichong and Liang, Chen and Chen, Weizhu and Zhao, Tuo},
  journal={arXiv preprint arXiv:2502.17410},
  year={2025}
}
@article{nguyen2025improving,
  title={Improving Adaptive Moment Optimization via Preconditioner Diagonalization},
  author={Nguyen, Son and Liu, Bo and Chen, Lizhang and Liu, Qiang},
  journal={arXiv preprint arXiv:2502.07488},
  year={2025}
}
@article{gupta2017unified,
  title={A unified approach to adaptive regularization in online and stochastic optimization},
  author={Gupta, Vineet and Koren, Tomer and Singer, Yoram},
  journal={arXiv preprint arXiv:1706.06569},
  year={2017}
}
@article{hazan2007logarithmic,
  title={Logarithmic regret algorithms for online convex optimization},
  author={Hazan, Elad and Agarwal, Amit and Kale, Satyen},
  journal={Machine Learning},
  volume={69},
  number={2},
  pages={169--192},
  year={2007},
  publisher={Springer}
}
@inproceedings{becker1988improving,
  title={Improving the convergence of backpropagation learning with second order method},
  author={Becker, Sue},
  booktitle={Proceedings of the 1988 Connectionist Models Summer School, San Mateo, CA},
  year={1988},
  organization={Morgan Kaufmann}
}
@inproceedings{schaul2013no,
  title={No more pesky learning rates},
  author={Schaul, Tom and Zhang, Sixin and LeCun, Yann},
  booktitle={International conference on machine learning},
  pages={343--351},
  year={2013},
  organization={PMLR}
}
@inproceedings{jahanidoubly,
  title={Doubly Adaptive Scaled Algorithm for Machine Learning Using Second-Order Information},
  author={Jahani, Majid and Rusakov, Sergey and Shi, Zheng and Richt{\'a}rik, Peter and Mahoney, Michael W and Takac, Martin},
  booktitle={International Conference on Learning Representations}
}
@article{amari1998natural,
  title={Natural gradient works efficiently in learning},
  author={Amari, Shun-Ichi},
  journal={Neural computation},
  volume={10},
  number={2},
  pages={251--276},
  year={1998},
  publisher={MIT Press}
}
@inproceedings{pascanu2013revisiting,
  author       = {Razvan Pascanu and
                  Yoshua Bengio},
  editor       = {Yoshua Bengio and
                  Yann LeCun},
  title        = {Revisiting Natural Gradient for Deep Networks},
  booktitle    = {2nd International Conference on Learning Representations, {ICLR} 2014,
                  Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings},
  year         = {2014},
}
@article{kavis2022adaptive,
  title={Adaptive stochastic variance reduction for non-convex finite-sum minimization},
  author={Kavis, Ali and Skoulakis, Stratis and Antonakopoulos, Kimon and Dadi, Leello Tadesse and Cevher, Volkan},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={23524--23538},
  year={2022}
}
@inproceedings{luo2019adaptive,
  author       = {Liangchen Luo and
                  Yuanhao Xiong and
                  Yan Liu and
                  Xu Sun},
  title        = {Adaptive Gradient Methods with Dynamic Bound of Learning Rate},
  booktitle    = {7th International Conference on Learning Representations, {ICLR} 2019,
                  New Orleans, LA, USA, May 6-9, 2019},
  year={2019}
}
@inproceedings{heo2020adamp,
  author       = {Byeongho Heo and
                  Sanghyuk Chun and
                  Seong Joon Oh and
                  Dongyoon Han and
                  Sangdoo Yun and
                  Gyuwan Kim and
                  Youngjung Uh and
                  Jung{-}Woo Ha},
  title        = {AdamP: Slowing Down the Slowdown for Momentum Optimizers on Scale-invariant
                  Weights},
  booktitle    = {9th International Conference on Learning Representations, {ICLR} 2021,
                  Virtual Event, Austria, May 3-7, 2021},
  year={2021}
}
@inproceedings{xie2022adaptive,
  title={Adaptive inertia: Disentangling the effects of adaptive learning rate and momentum},
  author={Xie, Zeke and Wang, Xinrui and Zhang, Huishuai and Sato, Issei and Sugiyama, Masashi},
  booktitle={International conference on machine learning},
  pages={24430--24459},
  year={2022},
  organization={PMLR}
}
@inproceedings{foret2020sharpness,
  author       = {Pierre Foret and
                  Ariel Kleiner and
                  Hossein Mobahi and
                  Behnam Neyshabur},
  title        = {Sharpness-aware Minimization for Efficiently Improving Generalization},
  booktitle    = {9th International Conference on Learning Representations, {ICLR} 2021,
                  Virtual Event, Austria, May 3-7, 2021},
  year         = {2021},
}
@inproceedings{kwon2021asam,
  title={Asam: Adaptive sharpness-aware minimization for scale-invariant learning of deep neural networks},
  author={Kwon, Jungmin and Kim, Jeongseop and Park, Hyunseo and Choi, In Kwon},
  booktitle={International Conference on Machine Learning},
  pages={5905--5914},
  year={2021},
  organization={PMLR}
}
@inproceedings{liu2022towards,
  title={Towards efficient and scalable sharpness-aware minimization},
  author={Liu, Yong and Mai, Siqi and Chen, Xiangning and Hsieh, Cho-Jui and You, Yang},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={12360--12370},
  year={2022}
}
@article{chen2018convergence,
  title={On the convergence of a class of adam-type algorithms for non-convex optimization},
  author={Chen, Xiangyi and Liu, Sijia and Sun, Ruoyu and Hong, Mingyi},
  journal={arXiv preprint arXiv:1808.02941},
  year={2018}
}
@article{liu2025muon,
  title={Muon is scalable for LLM training},
  author={Liu, Jingyuan and Su, Jianlin and Yao, Xingcheng and Jiang, Zhejun and Lai, Guokun and Du, Yulun and Qin, Yidao and Xu, Weixin and Lu, Enzhe and Yan, Junjie and others},
  journal={arXiv preprint arXiv:2502.16982},
  year={2025}
}
@article{wang2024cadam,
  title={CAdam: Confidence-Based Optimization for Online Learning},
  author={Wang, Shaowen and Liu, Anan and Xiao, Jian and Liu, Huan and Yang, Yuekui and Xu, Cong and Pu, Qianqian and Zheng, Suncong and Zhang, Wei and Wang, Di and others},
  journal={arXiv preprint arXiv:2411.19647},
  year={2024}
}
@article{lau2025polargrad,
  title={PolarGrad: A Class of Matrix-Gradient Optimizers from a Unifying Preconditioning Perspective},
  author={Lau, Tim Tsz-Kit and Long, Qi and Su, Weijie},
  journal={arXiv preprint arXiv:2505.21799},
  year={2025}
}
@article{amsel2025polar,
  title={The Polar Express: Optimal Matrix Sign Methods and Their Application to the Muon Algorithm},
  author={Amsel, Noah and Persson, David and Musco, Christopher and Gower, Robert},
  journal={arXiv preprint arXiv:2505.16932},
  year={2025}
}
@inproceedings{duvvuri2024combining,
  title={Combining axes preconditioners through kronecker approximation for deep learning},
  author={Duvvuri, Sai Surya and Devvrit, Fnu and Anil, Rohan and Hsieh, Cho-Jui and Dhillon, Inderjit S},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024}
}
@article{hazan2015beyond,
  title={Beyond convexity: Stochastic quasi-convex optimization},
  author={Hazan, Elad and Levy, Kfir and Shalev-Shwartz, Shai},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}
@inproceedings{cutkosky2020momentum,
  title={Momentum improves normalized sgd},
  author={Cutkosky, Ashok and Mehta, Harsh},
  booktitle={International conference on machine learning},
  pages={2260--2268},
  year={2020},
  organization={PMLR}
}
@article{pethick2025training,
  title={Training deep learning models with norm-constrained lmos},
  author={Pethick, Thomas and Xie, Wanyun and Antonakopoulos, Kimon and Zhu, Zhenyu and Silveti-Falls, Antonio and Cevher, Volkan},
  journal={arXiv preprint arXiv:2502.07529},
  year={2025}
}
@article{frank1956algorithm,
  title={An algorithm for quadratic programming},
  author={Frank, Marguerite and Wolfe, Philip and others},
  journal={Naval research logistics quarterly},
  volume={3},
  number={1-2},
  pages={95--110},
  year={1956},
  publisher={Wiley Subscription Services, Inc., A Wiley Company New York}
}
@article{clarkson2010coresets,
  title={Coresets, sparse greedy approximation, and the Frank-Wolfe algorithm},
  author={Clarkson, Kenneth L},
  journal={ACM Transactions on Algorithms (TALG)},
  volume={6},
  number={4},
  pages={1--30},
  year={2010},
  publisher={ACM New York, NY, USA}
}
@inproceedings{jaggi2013revisiting,
  title={Revisiting Frank-Wolfe: Projection-free sparse convex optimization},
  author={Jaggi, Martin},
  booktitle={International conference on machine learning},
  pages={427--435},
  year={2013},
  organization={PMLR}
}
@article{riabinin2025gluon,
  title={Gluon: Making Muon \& Scion Great Again!(Bridging Theory and Practice of LMO-based Optimizers for LLMs)},
  author={Riabinin, Artem and Shulgin, Egor and Gruntkowska, Kaja and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:2505.13416},
  year={2025}
}
@misc{kimi2025open,
  author       = {{Moonshot AI}},
  title        = {{Kimi K2: Open Agentic Intelligence}},
  year         = {2025},
  url          = {https://moonshotai.github.io/Kimi-K2/},
  note         = {Accessed: 2025-07-17}
}
@article{liu2024deepseek,
  title={Deepseek-v3 technical report},
  author={Liu, Aixin and Feng, Bei and Xue, Bing and Wang, Bingxuan and Wu, Bochao and Lu, Chengda and Zhao, Chenggang and Deng, Chengqi and Zhang, Chenyu and Ruan, Chong and others},
  journal={arXiv preprint arXiv:2412.19437},
  year={2024}
}
@article{team2025kimi,
  title={Kimi k1. 5: Scaling reinforcement learning with llms},
  author={Team, Kimi and Du, Angang and Gao, Bofei and Xing, Bowei and Jiang, Changjiu and Chen, Cheng and Li, Cheng and Xiao, Chenjun and Du, Chenzhuang and Liao, Chonghua and others},
  journal={arXiv preprint arXiv:2501.12599},
  year={2025}
}
@article{grattafiori2024llama,
  title={The llama 3 herd of models},
  author={Grattafiori, Aaron and Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Vaughan, Alex and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}
@article{ramesh2022hierarchical,
  title={Hierarchical text-conditional image generation with clip latents},
  author={Ramesh, Aditya and Dhariwal, Prafulla and Nichol, Alex and Chu, Casey and Chen, Mark},
  journal={arXiv preprint arXiv:2204.06125},
  volume={1},
  number={2},
  pages={3},
  year={2022}
}
@article{liu2023visual,
  title={Visual instruction tuning},
  author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
  journal={Advances in neural information processing systems},
  volume={36},
  pages={34892--34916},
  year={2023}
}
@article{wang2025gradpower,
  title={GradPower: Powering Gradients for Faster Language Model Pre-Training},
  author={Wang, Mingze and Wang, Jinbo and Zhang, Jiaqi and Wang, Wei and Pei, Peng and Cai, Xunliang and Wu, Lei and others},
  journal={arXiv preprint arXiv:2505.24275},
  year={2025}
}
@article{ahn2025dion,
  title={Dion: Distributed Orthonormalized Updates},
  author={Ahn, Kwangjun and Xu, Byron and Abreu, Natalie and Langford, John},
  journal={arXiv preprint arXiv:2504.05295},
  year={2025}
}
@article{zhang2020adaptive,
  title={Why are adaptive methods good for attention models?},
  author={Zhang, Jingzhao and Karimireddy, Sai Praneeth and Veit, Andreas and Kim, Seungyeon and Reddi, Sashank and Kumar, Sanjiv and Sra, Suvrit},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={15383--15393},
  year={2020}
}
@inproceedings{grosse2016kronecker,
  title={A kronecker-factored approximate fisher matrix for convolution layers},
  author={Grosse, Roger and Martens, James},
  booktitle={International Conference on Machine Learning},
  pages={573--582},
  year={2016},
  organization={PMLR}
}
@inproceedings{benzing2022gradient,
  title={Gradient descent on neurons and its link to approximate second-order optimization},
  author={Benzing, Frederik},
  booktitle={International conference on machine learning},
  pages={1817--1853},
  year={2022},
  organization={PMLR}
}