@INPROCEEDINGS{prop,
  author={Riedmiller, M. and Braun, H.},
  booktitle={IEEE International Conference on Neural Networks}, 
  title={A direct adaptive method for faster backpropagation learning: the RPROP algorithm}, 
  year={1993},
  volume={},
  number={},
  pages={586-591 vol.1},
  keywords={Backpropagation algorithms;Neurons;Acceleration;Supervised learning;Feedforward systems;Computer networks;Convergence;Writing},
  doi={10.1109/ICNN.1993.298623}}

@inproceedings{bernstein2018signsgd,
  title={signSGD: Compressed optimisation for non-convex problems},
  author={Bernstein, Jeremy and Wang, Yu-Xiang and Azizzadenesheli, Kamyar and Anandkumar, Animashree},
  booktitle={International Conference on Machine Learning},
  pages={560--569},
  year={2018},
  organization={PMLR}
}
@article{chen2024symbolic,
  title={Symbolic discovery of optimization algorithms},
  author={Chen, Xiangning and Liang, Chen and Huang, Da and Real, Esteban and Wang, Kaiyuan and Pham, Hieu and Dong, Xuanyi and Luong, Thang and Hsieh, Cho-Jui and Lu, Yifeng and others},
  journal={Advances in neural information processing systems},
  volume={36},
  year={2024}
}
@article{rumelhart1986learning,
  title={Learning representations by back-propagating errors},
  author={Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
  journal={nature},
  volume={323},
  number={6088},
  pages={533--536},
  year={1986},
  publisher={Nature Publishing Group UK London}
}
@inproceedings{nesterov1983method,
  title={A method for solving the convex programming problem with convergence rate O (1/k2)},
  author={Nesterov, Yurii},
  booktitle={Dokl akad nauk Sssr},
  volume={269},
  pages={543},
  year={1983}
}
@article{duchi2011adaptive,
  title={Adaptive subgradient methods for online learning and stochastic optimization.},
  author={Duchi, John and Hazan, Elad and Singer, Yoram},
  journal={Journal of machine learning research},
  volume={12},
  number={7},
  year={2011}
}
@article{zeiler2012adadelta,
  title={ADADELTA: an adaptive learning rate method},
  author={Zeiler, Matthew D},
  journal={arXiv preprint arXiv:1212.5701},
  year={2012}
}
@article{RMSProp,
	title={Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude},
	author={Tieleman, Tijmen and Hinton, Geoffrey},
	journal={COURSERA: Neural networks for machine learning},
	volume={4},
	number={2},
	pages={26--31},
	year={2012}
}
@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}
@article{loshchilov2017decoupled,
  title={Decoupled weight decay regularization},
  author={Loshchilov, I},
  journal={arXiv preprint arXiv:1711.05101},
  year={2017}
}
@article{reddi2019convergence,
  title={On the convergence of adam and beyond},
  author={Reddi, Sashank J and Kale, Satyen and Kumar, Sanjiv},
  journal={arXiv preprint arXiv:1904.09237},
  year={2019}
}
@article{dozat2016incorporating,
  title={Incorporating nesterov momentum into adam},
  author={Dozat, Timothy},
  year={2016}
}
@article{chen2018padam,
  title={Padam: Closing the generalization gap of adaptive gradient methods in training deep neural networks},
  author={Chen, Jinghui and Gu, Quanquan},
  year={2018}
}
@article{liu2020adam,
  title={Adam $^+$: A Stochastic Method with Adaptive Variance Reduction},
  author={Liu, Mingrui and Zhang, Wei and Orabona, Francesco and Yang, Tianbao},
  journal={arXiv preprint arXiv:2011.11985},
  year={2020}
}
@article{you2017large,
  title={Large batch training of convolutional networks},
  author={You, Yang and Gitman, Igor and Ginsburg, Boris},
  journal={arXiv preprint arXiv:1708.03888},
  year={2017}
}
@article{you2019large2,
  title={Large batch optimization for deep learning: Training bert in 76 minutes},
  author={You, Yang and Li, Jing and Reddi, Sashank and Hseu, Jonathan and Kumar, Sanjiv and Bhojanapalli, Srinadh and Song, Xiaodan and Demmel, James and Keutzer, Kurt and Hsieh, Cho-Jui},
  journal={arXiv preprint arXiv:1904.00962},
  year={2019}
}
@article{yuan2024mars,
  title={MARS: Unleashing the Power of Variance Reduction for Training Large Models},
  author={Yuan, Huizhuo and Liu, Yifeng and Wu, Shuang and Zhou, Xun and Gu, Quanquan},
  journal={arXiv preprint arXiv:2411.10438},
  year={2024}
}
@article{li2020adax,
  title={Adax: Adaptive gradient descent with exponential long term memory},
  author={Li, Wenjie and Zhang, Zhaoyang and Wang, Xinjiang and Luo, Ping},
  journal={arXiv preprint arXiv:2004.09740},
  year={2020}
}
@article{xie2024adan,
  title={Adan: Adaptive nesterov momentum algorithm for faster optimizing deep models},
  author={Xie, Xingyu and Zhou, Pan and Li, Huan and Lin, Zhouchen and Yan, Shuicheng},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2024},
  publisher={IEEE}
}
@article{defazio2024road,
  title={The road less scheduled},
  author={Defazio, Aaron and Yang, Xingyu Alice and Mehta, Harsh and Mishchenko, Konstantin and Khaled, Ahmed and Cutkosky, Ashok},
  journal={arXiv preprint arXiv:2405.15682},
  year={2024}
}
@article{loshchilov2023weight,
  title={Weight Norm Control},
  author={Loshchilov, Ilya},
  journal={arXiv preprint arXiv:2311.11446},
  year={2023}
}
@article{zhuang2020adabelief,
  title={Adabelief optimizer: Adapting stepsizes by the belief in observed gradients},
  author={Zhuang, Juntang and Tang, Tommy and Ding, Yifan and Tatikonda, Sekhar C and Dvornek, Nicha and Papademetris, Xenophon and Duncan, James},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={18795--18806},
  year={2020}
}
@inproceedings{li2022root,
  title={Root-sgd: Sharp nonasymptotics and asymptotic efficiency in a single algorithm},
  author={Li, Chris Junchi and Mou, Wenlong and Wainwright, Martin and Jordan, Michael},
  booktitle={Conference on Learning Theory},
  pages={909--981},
  year={2022},
  organization={PMLR}
}
@article{huang2021super,
  title={Super-adam: faster and universal framework of adaptive gradients},
  author={Huang, Feihu and Li, Junyi and Huang, Heng},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={9074--9085},
  year={2021}
}
@article{hutchinson1989stochastic,
  title={A stochastic estimator of the trace of the influence matrix for Laplacian smoothing splines},
  author={Hutchinson, Michael F},
  journal={Communications in Statistics-Simulation and Computation},
  volume={18},
  number={3},
  pages={1059--1076},
  year={1989},
  publisher={Taylor \& Francis}
}
@inproceedings{yao2021adahessian,
  title={Adahessian: An adaptive second order optimizer for machine learning},
  author={Yao, Zhewei and Gholami, Amir and Shen, Sheng and Mustafa, Mustafa and Keutzer, Kurt and Mahoney, Michael},
  booktitle={proceedings of the AAAI conference on artificial intelligence},
  volume={35},
  number={12},
  pages={10665--10673},
  year={2021}
}
@article{cutkosky2019momentum,
  title={Momentum-based variance reduction in non-convex sgd},
  author={Cutkosky, Ashok and Orabona, Francesco},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}
@article{levy2021storm+,
  title={Storm+: Fully adaptive sgd with recursive momentum for nonconvex optimization},
  author={Levy, Kfir and Kavis, Ali and Cevher, Volkan},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={20571--20582},
  year={2021}
}
@article{liu2023sophia,
  title={Sophia: A scalable stochastic second-order optimizer for language model pre-training},
  author={Liu, Hong and Li, Zhiyuan and Hall, David and Liang, Percy and Ma, Tengyu},
  journal={arXiv preprint arXiv:2305.14342},
  year={2023}
}
@article{tian2022amos,
  title={Amos: An adam-style optimizer with adaptive weight decay towards model-oriented scale},
  author={Tian, Ran and Parikh, Ankur P},
  journal={arXiv preprint arXiv:2210.11693},
  year={2022}
}
@inproceedings{martens2010deep,
  title={Deep learning via hessian-free optimization.},
  author={Martens, James and others},
  booktitle={Icml},
  volume={27},
  pages={735--742},
  year={2010}
}
@inproceedings{martens2015optimizing,
  title={Optimizing neural networks with kronecker-factored approximate curvature},
  author={Martens, James and Grosse, Roger},
  booktitle={International conference on machine learning},
  pages={2408--2417},
  year={2015},
  organization={PMLR}
}
@inproceedings{gupta2018shampoo,
  title={Shampoo: Preconditioned stochastic tensor optimization},
  author={Gupta, Vineet and Koren, Tomer and Singer, Yoram},
  booktitle={International Conference on Machine Learning},
  pages={1842--1850},
  year={2018},
  organization={PMLR}
}
@article{agarwal2020disentangling,
  title={Disentangling adaptive gradient methods from learning rates},
  author={Agarwal, Naman and Anil, Rohan and Hazan, Elad and Koren, Tomer and Zhang, Cyril},
  journal={arXiv preprint arXiv:2002.11803},
  year={2020}
}
@article{zhang2024adam,
  title={Adam-mini: Use fewer learning rates to gain more},
  author={Zhang, Yushun and Chen, Congliang and Li, Ziniu and Ding, Tian and Wu, Chenwei and Ye, Yinyu and Luo, Zhi-Quan and Sun, Ruoyu},
  journal={arXiv preprint arXiv:2406.16793},
  year={2024}
}
@article{zhao2024galore,
  title={Galore: Memory-efficient llm training by gradient low-rank projection},
  author={Zhao, Jiawei and Zhang, Zhenyu and Chen, Beidi and Wang, Zhangyang and Anandkumar, Anima and Tian, Yuandong},
  journal={arXiv preprint arXiv:2403.03507},
  year={2024}
}
@article{yang2022tensor,
  title={Tensor programs v: Tuning large neural networks via zero-shot hyperparameter transfer},
  author={Yang, Greg and Hu, Edward J and Babuschkin, Igor and Sidor, Szymon and Liu, Xiaodong and Farhi, David and Ryder, Nick and Pachocki, Jakub and Chen, Weizhu and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2203.03466},
  year={2022}
}
@misc{tigeropt,
  title={Tiger: A Tight-fisted Optimizer},
  author={Jianlin Su},
  year={2023},
  howpublished={\url{https://github.com/bojone/tiger}},
}
@inproceedings{defazio2023learning,
  title={Learning-rate-free learning by d-adaptation},
  author={Defazio, Aaron and Mishchenko, Konstantin},
  booktitle={International Conference on Machine Learning},
  pages={7449--7479},
  year={2023},
  organization={PMLR}
}
@article{bernstein2024old,
  title={Old optimizer, new norm: An anthology},
  author={Bernstein, Jeremy and Newhouse, Laker},
  journal={arXiv preprint arXiv:2409.20325},
  year={2024}
}
@misc{higham2008functions,
  title={Functions of Matrices. Society for Industrial and Applied Mathematics},
  author={Higham, Nicholas J},
  year={2008},
  publisher={SIAM Philadelphia},
  page={15}
}
@article{tran2019convergence,
  title={On the convergence proof of amsgrad and a new version},
  author={Tran, Phuong Thi and others},
  journal={IEEE Access},
  volume={7},
  pages={61706--61716},
  year={2019},
  publisher={IEEE}
}
@article{ward2020adagrad,
  title={Adagrad stepsizes: Sharp convergence over nonconvex landscapes},
  author={Ward, Rachel and Wu, Xiaoxia and Bottou, Leon},
  journal={Journal of Machine Learning Research},
  volume={21},
  number={219},
  pages={1--30},
  year={2020}
}
@article{jiang2024adaptive,
  title={Adaptive SGD with Polyak stepsize and line-search: Robust convergence and variance reduction},
  author={Jiang, Xiaowen and Stich, Sebastian U},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@misc{jordan2024muon,
  author       = {Keller Jordan and Yuchen Jin and Vlado Boza and Jiacheng You and
                  Franz Cecista and Laker Newhouse and Jeremy Bernstein},
  title        = {Muon: An optimizer for hidden layers in neural networks},
  year         = {2024},
  url          = {https://kellerjordan.github.io/posts/muon/}
}
@article{tran2019hybrid,
  title={Hybrid stochastic gradient descent algorithms for stochastic nonconvex optimization},
  author={Tran-Dinh, Quoc and Pham, Nhan H and Phan, Dzung T and Nguyen, Lam M},
  journal={arXiv preprint arXiv:1905.05920},
  year={2019}
}
@article{pagliardini2024ademamix,
  title={The ademamix optimizer: Better, faster, older},
  author={Pagliardini, Matteo and Ablin, Pierre and Grangier, David},
  journal={arXiv preprint arXiv:2409.03137},
  year={2024}
}
@article{armijo1966minimization,
  title={Minimization of functions having Lipschitz continuous first partial derivatives},
  author={Armijo, Larry},
  journal={Pacific Journal of mathematics},
  volume={16},
  number={1},
  pages={1--3},
  year={1966},
  publisher={Mathematical Sciences Publishers}
}
@article{scheithauerjorge,
  title={Numerical Optimization},
  author={Jorge Nocedal and Stephen J. Wright},
  journal={},
  volume ={Springer, New York, NY, USA, 2e edition},
  year={2006}
}
@article{vyas2024soap,
  title={Soap: Improving and stabilizing shampoo using adam},
  author={Vyas, Nikhil and Morwani, Depen and Zhao, Rosie and Shapira, Itai and Brandfonbrener, David and Janson, Lucas and Kakade, Sham},
  journal={arXiv preprint arXiv:2409.11321},
  year={2024}
}
@article{roux2012stochastic,
  title={A stochastic gradient method with an exponential convergence \_rate for finite training sets},
  author={Roux, Nicolas and Schmidt, Mark and Bach, Francis},
  journal={Advances in neural information processing systems},
  volume={25},
  year={2012}
}
@article{shalev2013stochastic,
  title={Stochastic dual coordinate ascent methods for regularized loss minimization.},
  author={Shalev-Shwartz, Shai and Zhang, Tong},
  journal={Journal of Machine Learning Research},
  volume={14},
  number={1},
  year={2013}
}
@article{johnson2013accelerating,
  title={Accelerating stochastic gradient descent using predictive variance reduction},
  author={Johnson, Rie and Zhang, Tong},
  journal={Advances in neural information processing systems},
  volume={26},
  year={2013}
}
@article{defazio2014saga,
  title={SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives},
  author={Defazio, Aaron and Bach, Francis and Lacoste-Julien, Simon},
  journal={Advances in neural information processing systems},
  volume={27},
  year={2014}
}
@inproceedings{nguyen2017sarah,
  title={SARAH: A novel method for machine learning problems using stochastic recursive gradient},
  author={Nguyen, Lam M and Liu, Jie and Scheinberg, Katya and Tak{\'a}{\v{c}}, Martin},
  booktitle={International conference on machine learning},
  pages={2613--2621},
  year={2017},
  organization={PMLR}
}
@article{fang2018spider,
  title={Spider: Near-optimal non-convex optimization via stochastic path-integrated differential estimator},
  author={Fang, Cong and Li, Chris Junchi and Lin, Zhouchen and Zhang, Tong},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}
@article{zhou2020stochastic,
  title={Stochastic nested variance reduction for nonconvex optimization},
  author={Zhou, Dongruo and Xu, Pan and Gu, Quanquan},
  journal={Journal of machine learning research},
  volume={21},
  number={103},
  pages={1--63},
  year={2020}
}
@article{wang2019spiderboost,
  title={Spiderboost and momentum: Faster variance reduction algorithms},
  author={Wang, Zhe and Ji, Kaiyi and Zhou, Yi and Liang, Yingbin and Tarokh, Vahid},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}
@inproceedings{shazeer2018adafactor,
  title={Adafactor: Adaptive learning rates with sublinear memory cost},
  author={Shazeer, Noam and Stern, Mitchell},
  booktitle={International Conference on Machine Learning},
  pages={4596--4604},
  year={2018},
  organization={PMLR}
}
@article{polyak1992acceleration,
  title={Acceleration of stochastic approximation by averaging},
  author={Polyak, Boris T and Juditsky, Anatoli B},
  journal={SIAM journal on control and optimization},
  volume={30},
  number={4},
  pages={838--855},
  year={1992},
  publisher={SIAM}
}
@article{liu1989limited,
  title={On the limited memory BFGS method for large scale optimization},
  author={Liu, Dong C and Nocedal, Jorge},
  journal={Mathematical programming},
  volume={45},
  number={1},
  pages={503--528},
  year={1989},
  publisher={Springer}
}
@article{zaheer2018adaptive,
  title={Adaptive methods for nonconvex optimization},
  author={Zaheer, Manzil and Reddi, Sashank and Sachan, Devendra and Kale, Satyen and Kumar, Sanjiv},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}
@article{zhang2021dive,
  title={Dive into deep learning},
  author={Zhang, Aston and Lipton, Zachary C and Li, Mu and Smola, Alexander J},
  journal={arXiv preprint arXiv:2106.11342},
  year={2021},
  pages={505}
}
@article{liu2019variance,
  title={On the variance of the adaptive learning rate and beyond},
  author={Liu, Liyuan and Jiang, Haoming and He, Pengcheng and Chen, Weizhu and Liu, Xiaodong and Gao, Jianfeng and Han, Jiawei},
  journal={arXiv preprint arXiv:1908.03265},
  year={2019}
}
@article{zhao2024deconstructing,
  title={Deconstructing what makes a good optimizer for language models},
  author={Zhao, Rosie and Morwani, Depen and Brandfonbrener, David and Vyas, Nikhil and Kakade, Sham},
  journal={arXiv preprint arXiv:2407.07972},
  year={2024}
}
@article{yuan2020stochastic,
  title={Stochastic recursive momentum for policy gradient methods},
  author={Yuan, Huizhuo and Lian, Xiangru and Liu, Ji and Zhou, Yuren},
  journal={arXiv preprint arXiv:2003.04302},
  year={2020}
}
@book{fletcher1987practical,
  title={Practical Methods of Optimization},
  author={Fletcher, Roger},
  year={1987},
  edition={2},
  publisher={John Wiley \& Sons},
  address={New York},
  isbn={978-0-471-91547-8}
}
@inproceedings{dalalyan2017further,
  title={Further and stronger analogy between sampling and optimization: Langevin Monte Carlo and gradient descent},
  author={Dalalyan, Arnak},
  booktitle={Conference on Learning Theory},
  pages={678--689},
  year={2017},
  organization={PMLR}
}
@article{dalalyan2017theoretical,
  title={Theoretical guarantees for approximate sampling from smooth and log-concave densities},
  author={Dalalyan, Arnak S},
  journal={Journal of the Royal Statistical Society Series B: Statistical Methodology},
  volume={79},
  number={3},
  pages={651--676},
  year={2017},
  publisher={Oxford University Press}
}
@article{durmus2017nonasymptotic,
  title={Nonasymptotic convergence analysis for the unadjusted Langevin algorithm},
  author={Durmus, Alain and Moulines, Eric},
  year={2017}
}
@inproceedings{welling2011bayesian,
  title={Bayesian learning via stochastic gradient Langevin dynamics},
  author={Welling, Max and Teh, Yee W},
  booktitle={Proceedings of the 28th international conference on machine learning (ICML-11)},
  pages={681--688},
  year={2011},
  organization={Citeseer}
}
@article{xu2018global,
  title={Global convergence of Langevin dynamics based algorithms for nonconvex optimization},
  author={Xu, Pan and Chen, Jinghui and Zou, Difan and Gu, Quanquan},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}
@article{ahn2012bayesian,
  title={Bayesian posterior sampling via stochastic gradient Fisher scoring},
  author={Ahn, Sungjin and Korattikara, Anoop and Welling, Max},
  journal={arXiv preprint arXiv:1206.6380},
  year={2012}
}
@article{wu2024self,
  title={Self-play preference optimization for language model alignment},
  author={Wu, Yue and Sun, Zhiqing and Yuan, Huizhuo and Ji, Kaixuan and Yang, Yiming and Gu, Quanquan},
  journal={arXiv preprint arXiv:2405.00675},
  year={2024}
}
@misc{tao2024simpleprovableparameterfreeadaptive,
      title={Towards Simple and Provable Parameter-Free Adaptive Gradient Methods}, 
      author={Yuanzhe Tao and Huizhuo Yuan and Xun Zhou and Yuan Cao and Quanquan Gu},
      year={2024},
      eprint={2412.19444},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2412.19444}, 
}
@article{liang2024cautious,
  title={Cautious Optimizers: Improving Training with One Line of Code},
  author={Liang, Kaizhao and Chen, Lizhang and Liu, Bo and Liu, Qiang},
  journal={arXiv preprint arXiv:2411.16085},
  year={2024}
}
@article{cao2024grams,
  title={Grams: Gradient Descent with Adaptive Momentum Scaling},
  author={Cao, Yang and Li, Xiaoyu and Song, Zhao},
  journal={arXiv preprint arXiv:2412.17107},
  year={2024}
}
@article{defazio2023and,
  title={When, why and how much? adaptive learning rate scheduling by refinement},
  author={Defazio, Aaron and Cutkosky, Ashok and Mehta, Harsh and Mishchenko, Konstantin},
  journal={arXiv preprint arXiv:2310.07831},
  year={2023}
}
@article{hu2024minicpm,
  title={Minicpm: Unveiling the potential of small language models with scalable training strategies},
  author={Hu, Shengding and Tu, Yuge and Han, Xu and He, Chaoqun and Cui, Ganqu and Long, Xiang and Zheng, Zhi and Fang, Yewei and Huang, Yuxiang and Zhao, Weilin and others},
  journal={arXiv preprint arXiv:2404.06395},
  year={2024}
}
@article{wen2024understanding,
  title={Understanding Warmup-Stable-Decay Learning Rates: A River Valley Loss Landscape Perspective},
  author={Wen, Kaiyue and Li, Zhiyuan and Wang, Jason and Hall, David and Liang, Percy and Ma, Tengyu},
  journal={arXiv preprint arXiv:2410.05192},
  year={2024}
}